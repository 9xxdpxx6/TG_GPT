import json
import os
import torch
from pathlib import Path
from transformers import (
    AutoTokenizer, 
    AutoModelForCausalLM, 
    TrainingArguments, 
    Trainer, 
    DataCollatorForLanguageModeling
)
from datasets import Dataset
import logging

# –ù–∞—Å—Ç—Ä–æ–π–∫–∞ –ª–æ–≥–∏—Ä–æ–≤–∞–Ω–∏—è
logging.basicConfig(level=logging.INFO)
logger = logging.getLogger(__name__)

def load_training_data(dataset_path="training_dataset.json"):
    """–ó–∞–≥—Ä—É–∂–∞–µ—Ç –ø–æ–¥–≥–æ—Ç–æ–≤–ª–µ–Ω–Ω—ã–π –¥–∞—Ç–∞—Å–µ—Ç"""
    logger.info(f"–ó–∞–≥—Ä—É–∂–∞—é –¥–∞—Ç–∞—Å–µ—Ç –∏–∑ {dataset_path}")
    
    if not os.path.exists(dataset_path):
        raise FileNotFoundError(f"–§–∞–π–ª {dataset_path} –Ω–µ –Ω–∞–π–¥–µ–Ω! –°–Ω–∞—á–∞–ª–∞ –∑–∞–ø—É—Å—Ç–∏—Ç–µ prepare_training_dataset.py")
    
    with open(dataset_path, 'r', encoding='utf-8') as f:
        data = json.load(f)
    
    logger.info(f"–ó–∞–≥—Ä—É–∂–µ–Ω–æ {len(data)} –¥–∏–∞–ª–æ–≥–æ–≤")
    return data

def prepare_dataset_for_training(data):
    """–ü–æ–¥–≥–æ—Ç–∞–≤–ª–∏–≤–∞–µ—Ç –¥–∞—Ç–∞—Å–µ—Ç –¥–ª—è –æ–±—É—á–µ–Ω–∏—è"""
    logger.info("–ü–æ–¥–≥–æ—Ç–∞–≤–ª–∏–≤–∞—é –¥–∞—Ç–∞—Å–µ—Ç –¥–ª—è –æ–±—É—á–µ–Ω–∏—è...")
    
    # –ü—Ä–µ–æ–±—Ä–∞–∑—É–µ–º –≤ —Ñ–æ—Ä–º–∞—Ç Dataset
    dataset = Dataset.from_list(data)
    
    # –ó–∞–≥—Ä—É–∂–∞–µ–º —Ç–æ–∫–µ–Ω–∏–∑–∞—Ç–æ—Ä
    model_name = "microsoft/DialoGPT-small"
    logger.info(f"–ó–∞–≥—Ä—É–∂–∞—é —Ç–æ–∫–µ–Ω–∏–∑–∞—Ç–æ—Ä: {model_name}")
    
    tokenizer = AutoTokenizer.from_pretrained(model_name)
    tokenizer.pad_token = tokenizer.eos_token
    
    def format_dataset(examples):
        """–§–æ—Ä–º–∞—Ç–∏—Ä—É–µ—Ç –¥–∞–Ω–Ω—ã–µ –¥–ª—è –æ–±—É—á–µ–Ω–∏—è"""
        inputs = []
        for i in range(len(examples['context'])):
            context = examples['context'][i]
            response = examples['response'][i]
            
            # –§–æ—Ä–º–∏—Ä—É–µ–º —Ç–µ–∫—Å—Ç –≤ —Ñ–æ—Ä–º–∞—Ç–µ –¥–∏–∞–ª–æ–≥–∞
            dialog_text = ""
            for msg in context:
                dialog_text += f"–°–æ–±–µ—Å–µ–¥–Ω–∏–∫: {msg}\n"
            dialog_text += f"–í—ã: {response}{tokenizer.eos_token}"
            
            inputs.append(dialog_text)
        
        # –¢–æ–∫–µ–Ω–∏–∑–∏—Ä—É–µ–º —Å –æ–≥—Ä–∞–Ω–∏—á–µ–Ω–∏–µ–º –¥–ª–∏–Ω—ã
        model_inputs = tokenizer(
            inputs, 
            max_length=256,  # –£–º–µ–Ω—å—à–∞–µ–º –¥–ª—è —ç–∫–æ–Ω–æ–º–∏–∏ –ø–∞–º—è—Ç–∏
            truncation=True, 
            padding="max_length"
        )
        model_inputs["labels"] = model_inputs["input_ids"].copy()
        return model_inputs
    
    # –ü—Ä–∏–º–µ–Ω—è–µ–º —Ñ–æ—Ä–º–∞—Ç–∏—Ä–æ–≤–∞–Ω–∏–µ
    logger.info("–¢–æ–∫–µ–Ω–∏–∑–∏—Ä—É—é –¥–∞—Ç–∞—Å–µ—Ç...")
    tokenized_dataset = dataset.map(
        format_dataset, 
        batched=True, 
        remove_columns=dataset.column_names
    )
    
    # –†–∞–∑–¥–µ–ª—è–µ–º –Ω–∞ –æ–±—É—á–µ–Ω–∏–µ –∏ –≤–∞–ª–∏–¥–∞—Ü–∏—é
    split_dataset = tokenized_dataset.train_test_split(test_size=0.1)
    logger.info(f"–†–∞–∑–¥–µ–ª–µ–Ω–æ –Ω–∞ {len(split_dataset['train'])} –æ–±—É—á–∞—é—â–∏—Ö –∏ {len(split_dataset['test'])} —Ç–µ—Å—Ç–æ–≤—ã—Ö –ø—Ä–∏–º–µ—Ä–æ–≤")
    
    return tokenized_dataset, split_dataset, tokenizer

def load_model_and_tokenizer(model_name="microsoft/DialoGPT-small"):
    """–ó–∞–≥—Ä—É–∂–∞–µ—Ç –º–æ–¥–µ–ª—å –∏ —Ç–æ–∫–µ–Ω–∏–∑–∞—Ç–æ—Ä"""
    logger.info(f"–ó–∞–≥—Ä—É–∂–∞—é –º–æ–¥–µ–ª—å: {model_name}")
    
    tokenizer = AutoTokenizer.from_pretrained(model_name)
    model = AutoModelForCausalLM.from_pretrained(model_name)
    
    # –î–æ–±–∞–≤–ª—è–µ–º —Å–ø–µ—Ü–∏–∞–ª—å–Ω—ã–π —Ç–æ–∫–µ–Ω
    tokenizer.pad_token = tokenizer.eos_token
    
    return model, tokenizer

def setup_training_arguments(output_dir="./results"):
    """–ù–∞—Å—Ç—Ä–∞–∏–≤–∞–µ—Ç –ø–∞—Ä–∞–º–µ—Ç—Ä—ã –æ–±—É—á–µ–Ω–∏—è –¥–ª—è –ª–æ–∫–∞–ª—å–Ω–æ–≥–æ –∫–æ–º–ø—å—é—Ç–µ—Ä–∞"""
    logger.info("–ù–∞—Å—Ç—Ä–∞–∏–≤–∞—é –ø–∞—Ä–∞–º–µ—Ç—Ä—ã –æ–±—É—á–µ–Ω–∏—è –¥–ª—è –ª–æ–∫–∞–ª—å–Ω–æ–≥–æ –∫–æ–º–ø—å—é—Ç–µ—Ä–∞...")
    
    return TrainingArguments(
        output_dir=output_dir,
        num_train_epochs=3,                    # –ù–∞—á–∏–Ω–∞–µ–º —Å 2 —ç–ø–æ—Ö
        per_device_train_batch_size=2,         # –ú–∞–ª–µ–Ω—å–∫–∏–π batch size –¥–ª—è —ç–∫–æ–Ω–æ–º–∏–∏ –ø–∞–º—è—Ç–∏
        per_device_eval_batch_size=2,
        gradient_accumulation_steps=4,         # –≠–∫–æ–Ω–æ–º–∏—è –ø–∞–º—è—Ç–∏
        warmup_steps=50,                       # –ë—ã—Å—Ç—Ä—ã–π —Å—Ç–∞—Ä—Ç
        weight_decay=0.01,
        logging_dir='./logs',
        logging_steps=25,                      # –ß–∞—â–µ –ª–æ–≥–∏ –¥–ª—è –∫–æ–Ω—Ç—Ä–æ–ª—è
        save_strategy="epoch",
        load_best_model_at_end=True,
        report_to="none",                      # –û—Ç–∫–ª—é—á–∞–µ–º –≤–Ω–µ—à–Ω–∏–µ —Å–µ—Ä–≤–∏—Å—ã
        save_total_limit=2,                    # –û–≥—Ä–∞–Ω–∏—á–∏–≤–∞–µ–º –∫–æ–ª–∏—á–µ—Å—Ç–≤–æ —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∏–π
        dataloader_pin_memory=False,           # –≠–∫–æ–Ω–æ–º–∏—è –ø–∞–º—è—Ç–∏
        dataloader_num_workers=0,              # –î–ª—è Windows
    )

def train_model(model, tokenizer, train_dataset, eval_dataset, training_args):
    """–û–±—É—á–∞–µ—Ç –º–æ–¥–µ–ª—å"""
    logger.info("–ù–∞—á–∏–Ω–∞—é –æ–±—É—á–µ–Ω–∏–µ –º–æ–¥–µ–ª–∏...")
    
    # –°–æ–∑–¥–∞–µ–º data collator
    data_collator = DataCollatorForLanguageModeling(
        tokenizer=tokenizer,
        mlm=False
    )
    
    # –°–æ–∑–¥–∞–µ–º Trainer
    trainer = Trainer(
        model=model,
        args=training_args,
        train_dataset=train_dataset,
        eval_dataset=eval_dataset,
        data_collator=data_collator,
    )
    
    # –ó–∞–ø—É—Å–∫ –æ–±—É—á–µ–Ω–∏—è
    logger.info("üöÄ –ó–∞–ø—É—Å–∫–∞—é –æ–±—É—á–µ–Ω–∏–µ...")
    logger.info("–≠—Ç–æ –º–æ–∂–µ—Ç –∑–∞–Ω—è—Ç—å 1-3 —á–∞—Å–∞ –Ω–∞ –ª–æ–∫–∞–ª—å–Ω–æ–º –∫–æ–º–ø—å—é—Ç–µ—Ä–µ")
    logger.info("–ú–æ–∂–µ—Ç–µ –æ—Å—Ç–∞–≤–∏—Ç—å –∫–æ–º–ø—å—é—Ç–µ—Ä —Ä–∞–±–æ—Ç–∞—Ç—å –∏ –≤–µ—Ä–Ω—É—Ç—å—Å—è –ø–æ–∑–∂–µ")
    
    trainer.train()
    
    return trainer

def save_model(trainer, tokenizer, output_dir="./my_dialogpt"):
    """–°–æ—Ö—Ä–∞–Ω—è–µ—Ç –æ–±—É—á–µ–Ω–Ω—É—é –º–æ–¥–µ–ª—å"""
    logger.info(f"–°–æ—Ö—Ä–∞–Ω—è—é –º–æ–¥–µ–ª—å –≤ {output_dir}")
    
    # –°–æ–∑–¥–∞–µ–º –¥–∏—Ä–µ–∫—Ç–æ—Ä–∏—é
    os.makedirs(output_dir, exist_ok=True)
    
    # –°–æ—Ö—Ä–∞–Ω—è–µ–º –º–æ–¥–µ–ª—å
    trainer.save_model(output_dir)
    tokenizer.save_pretrained(output_dir)
    
    logger.info(f"‚úÖ –ú–æ–¥–µ–ª—å —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∞ –≤ {output_dir}")
    logger.info("–¢–µ–ø–µ—Ä—å –º–æ–∂–µ—Ç–µ –∏—Å–ø–æ–ª—å–∑–æ–≤–∞—Ç—å –µ—ë –¥–ª—è –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏ —Å–æ–æ–±—â–µ–Ω–∏–π!")

def main():
    """–û—Å–Ω–æ–≤–Ω–∞—è —Ñ—É–Ω–∫—Ü–∏—è"""
    print("ü§ñ –õ–æ–∫–∞–ª—å–Ω–æ–µ –æ–±—É—á–µ–Ω–∏–µ –º–æ–¥–µ–ª–∏ DialoGPT –Ω–∞ –≤–∞—à–∏—Ö –¥–∞–Ω–Ω—ã—Ö")
    print("=" * 60)
    
    try:
        # 1. –ó–∞–≥—Ä—É–∂–∞–µ–º –¥–∞–Ω–Ω—ã–µ
        data = load_training_data()
        
        # 2. –ü–æ–¥–≥–æ—Ç–∞–≤–ª–∏–≤–∞–µ–º –¥–∞—Ç–∞—Å–µ—Ç
        tokenized_dataset, split_dataset, tokenizer = prepare_dataset_for_training(data)
        
        # 3. –ó–∞–≥—Ä—É–∂–∞–µ–º –º–æ–¥–µ–ª—å
        model, tokenizer = load_model_and_tokenizer()
        
        # 4. –ù–∞—Å—Ç—Ä–∞–∏–≤–∞–µ–º –ø–∞—Ä–∞–º–µ—Ç—Ä—ã –æ–±—É—á–µ–Ω–∏—è
        training_args = setup_training_arguments()
        
        # 5. –û–±—É—á–∞–µ–º –º–æ–¥–µ–ª—å
        trainer = train_model(
            model, tokenizer, 
            split_dataset["train"], 
            split_dataset["test"], 
            training_args
        )
        
        # 6. –°–æ—Ö—Ä–∞–Ω—è–µ–º —Ä–µ–∑—É–ª—å—Ç–∞—Ç
        save_model(trainer, tokenizer)
        
        print("\nüéâ –û–±—É—á–µ–Ω–∏–µ –∑–∞–≤–µ—Ä—à–µ–Ω–æ —É—Å–ø–µ—à–Ω–æ!")
        print("üìÅ –ú–æ–¥–µ–ª—å —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∞ –≤ –ø–∞–ø–∫–µ './my_dialogpt'")
        print("üîß –¢–µ–ø–µ—Ä—å –º–æ–∂–µ—Ç–µ –∑–∞–ø—É—Å—Ç–∏—Ç—å generator.py –¥–ª—è —Ç–µ—Å—Ç–∏—Ä–æ–≤–∞–Ω–∏—è!")
        
    except Exception as e:
        logger.error(f"–û—à–∏–±–∫–∞ –ø—Ä–∏ –æ–±—É—á–µ–Ω–∏–∏: {e}")
        print(f"\n‚ùå –ü—Ä–æ–∏–∑–æ—à–ª–∞ –æ—à–∏–±–∫–∞: {e}")
        print("–ü—Ä–æ–≤–µ—Ä—å—Ç–µ:")
        print("1. –£—Å—Ç–∞–Ω–æ–≤–ª–µ–Ω—ã –ª–∏ –≤—Å–µ –±–∏–±–ª–∏–æ—Ç–µ–∫–∏ (pip install transformers torch datasets)")
        print("2. –ï—Å—Ç—å –ª–∏ —Ñ–∞–π–ª training_dataset.json")
        print("3. –î–æ—Å—Ç–∞—Ç–æ—á–Ω–æ –ª–∏ –º–µ—Å—Ç–∞ –Ω–∞ –¥–∏—Å–∫–µ")

if __name__ == "__main__":
    main() 