import torch
import json
from transformers import AutoModelForCausalLM, AutoTokenizer
import logging

# –ù–∞—Å—Ç—Ä–æ–π–∫–∞ –ª–æ–≥–∏—Ä–æ–≤–∞–Ω–∏—è
logging.basicConfig(level=logging.INFO)
logger = logging.getLogger(__name__)

class MessageGenerator:
    """–ì–µ–Ω–µ—Ä–∞—Ç–æ—Ä —Å–æ–æ–±—â–µ–Ω–∏–π –Ω–∞ –æ—Å–Ω–æ–≤–µ –æ–±—É—á–µ–Ω–Ω–æ–π –º–æ–¥–µ–ª–∏"""
    
    def __init__(self, model_path="../dataset/my_dialogpt"):
        """–ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏—è –≥–µ–Ω–µ—Ä–∞—Ç–æ—Ä–∞"""
        self.model_path = model_path
        self.tokenizer = None
        self.model = None
        self.device = "cuda" if torch.cuda.is_available() else "cpu"
        
        logger.info(f"–ò—Å–ø–æ–ª—å–∑—É–µ—Ç—Å—è —É—Å—Ç—Ä–æ–π—Å—Ç–≤–æ: {self.device}")
        self.load_model()
    
    def load_model(self):
        """–ó–∞–≥—Ä—É–∂–∞–µ—Ç –æ–±—É—á–µ–Ω–Ω—É—é –º–æ–¥–µ–ª—å"""
        try:
            logger.info(f"–ó–∞–≥—Ä—É–∂–∞—é –º–æ–¥–µ–ª—å –∏–∑ {self.model_path}")
            
            if not torch.cuda.is_available():
                logger.warning("CUDA –Ω–µ–¥–æ—Å—Ç—É–ø–Ω–∞, –∏—Å–ø–æ–ª—å–∑—É–µ—Ç—Å—è CPU. –û–±—É—á–µ–Ω–∏–µ –±—É–¥–µ—Ç –º–µ–¥–ª–µ–Ω–Ω–µ–µ.")
            
            # –ó–∞–≥—Ä—É–∂–∞–µ–º —Ç–æ–∫–µ–Ω–∏–∑–∞—Ç–æ—Ä –∏ –º–æ–¥–µ–ª—å
            self.tokenizer = AutoTokenizer.from_pretrained(self.model_path)
            self.model = AutoModelForCausalLM.from_pretrained(
                self.model_path,
                torch_dtype=torch.float32 if self.device == "cpu" else torch.float16
            )
            
            # –ü–µ—Ä–µ–º–µ—â–∞–µ–º –º–æ–¥–µ–ª—å –Ω–∞ —É—Å—Ç—Ä–æ–π—Å—Ç–≤–æ
            self.model.to(self.device)
            self.model.eval()
            
            logger.info("‚úÖ –ú–æ–¥–µ–ª—å —É—Å–ø–µ—à–Ω–æ –∑–∞–≥—Ä—É–∂–µ–Ω–∞!")
            
        except Exception as e:
            logger.error(f"–û—à–∏–±–∫–∞ –ø—Ä–∏ –∑–∞–≥—Ä—É–∑–∫–µ –º–æ–¥–µ–ª–∏: {e}")
            raise
    
    def generate_response(self, context_messages, max_length=100, temperature=0.7):
        """
        –ì–µ–Ω–µ—Ä–∏—Ä—É–µ—Ç –æ—Ç–≤–µ—Ç –Ω–∞ –æ—Å–Ω–æ–≤–µ –∫–æ–Ω—Ç–µ–∫—Å—Ç–∞
        
        –ê—Ä–≥—É–º–µ–Ω—Ç—ã:
        context_messages -- —Å–ø–∏—Å–æ–∫ –ø–æ—Å–ª–µ–¥–Ω–∏—Ö —Å–æ–æ–±—â–µ–Ω–∏–π —Å–æ–±–µ—Å–µ–¥–Ω–∏–∫–∞
        max_length -- –º–∞–∫—Å–∏–º–∞–ª—å–Ω–∞—è –¥–ª–∏–Ω–∞ –æ—Ç–≤–µ—Ç–∞
        temperature -- –∫—Ä–µ–∞—Ç–∏–≤–Ω–æ—Å—Ç—å (0.1-1.0, –≥–¥–µ 1.0 - —Å–∞–º–∞—è –∫—Ä–µ–∞—Ç–∏–≤–Ω–∞—è)
        
        –í–æ–∑–≤—Ä–∞—â–∞–µ—Ç:
        –°–≥–µ–Ω–µ—Ä–∏—Ä–æ–≤–∞–Ω–Ω—ã–π –æ—Ç–≤–µ—Ç
        """
        if not self.model or not self.tokenizer:
            raise RuntimeError("–ú–æ–¥–µ–ª—å –Ω–µ –∑–∞–≥—Ä—É–∂–µ–Ω–∞!")
        
        try:
            # –§–æ—Ä–º–∏—Ä—É–µ–º –≤—Ö–æ–¥–Ω–æ–π —Ç–µ–∫—Å—Ç
            dialog_text = ""
            for msg in context_messages:
                dialog_text += f"–°–æ–±–µ—Å–µ–¥–Ω–∏–∫: {msg}\n"
            dialog_text += "–í—ã: "
            
            logger.info(f"–ì–µ–Ω–µ—Ä–∏—Ä—É—é –æ—Ç–≤–µ—Ç –¥–ª—è –∫–æ–Ω—Ç–µ–∫—Å—Ç–∞: {context_messages}")
            
            # –¢–æ–∫–µ–Ω–∏–∑–∏—Ä—É–µ–º
            inputs = self.tokenizer(
                dialog_text, 
                return_tensors="pt", 
                max_length=256, 
                truncation=True
            ).to(self.device)
            
            # –ì–µ–Ω–µ—Ä–∏—Ä—É–µ–º –æ—Ç–≤–µ—Ç
            with torch.no_grad():
                reply_ids = self.model.generate(
                    inputs["input_ids"],
                    max_length=inputs["input_ids"].shape[1] + max_length,
                    do_sample=True,
                    top_p=0.95,
                    top_k=50,
                    temperature=temperature,
                    pad_token_id=self.tokenizer.eos_token_id,
                    repetition_penalty=1.1,  # –ò–∑–±–µ–≥–∞–µ–º –ø–æ–≤—Ç–æ—Ä–µ–Ω–∏–π
                    no_repeat_ngram_size=3   # –ò–∑–±–µ–≥–∞–µ–º –ø–æ–≤—Ç–æ—Ä–µ–Ω–∏—è —Ñ—Ä–∞–∑
                )
            
            # –î–µ–∫–æ–¥–∏—Ä—É–µ–º –∏ –æ—á–∏—â–∞–µ–º –æ—Ç–≤–µ—Ç
            full_response = self.tokenizer.decode(reply_ids[0], skip_special_tokens=True)
            
            # –ò–∑–≤–ª–µ–∫–∞–µ–º —Ç–æ–ª—å–∫–æ –≤–∞—à –æ—Ç–≤–µ—Ç
            if "–í—ã: " in full_response:
                your_response = full_response.split("–í—ã: ")[-1].strip()
            else:
                your_response = full_response.strip()
            
            logger.info(f"–°–≥–µ–Ω–µ—Ä–∏—Ä–æ–≤–∞–Ω –æ—Ç–≤–µ—Ç: {your_response}")
            return your_response
            
        except Exception as e:
            logger.error(f"–û—à–∏–±–∫–∞ –ø—Ä–∏ –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏: {e}")
            return "–ò–∑–≤–∏–Ω–∏—Ç–µ, –Ω–µ —É–¥–∞–ª–æ—Å—å —Å–≥–µ–Ω–µ—Ä–∏—Ä–æ–≤–∞—Ç—å –æ—Ç–≤–µ—Ç."
    
    def generate_multiple_responses(self, context_messages, num_responses=3, temperature=0.7):
        """–ì–µ–Ω–µ—Ä–∏—Ä—É–µ—Ç –Ω–µ—Å–∫–æ–ª—å–∫–æ –≤–∞—Ä–∏–∞–Ω—Ç–æ–≤ –æ—Ç–≤–µ—Ç–∞"""
        responses = []
        
        for i in range(num_responses):
            # –ù–µ–º–Ω–æ–≥–æ –≤–∞—Ä—å–∏—Ä—É–µ–º —Ç–µ–º–ø–µ—Ä–∞—Ç—É—Ä—É –¥–ª—è —Ä–∞–∑–Ω–æ–æ–±—Ä–∞–∑–∏—è
            temp = temperature + (i * 0.1)
            response = self.generate_response(context_messages, temperature=temp)
            responses.append(response)
        
        return responses

def interactive_mode():
    """–ò–Ω—Ç–µ—Ä–∞–∫—Ç–∏–≤–Ω—ã–π —Ä–µ–∂–∏–º –¥–ª—è —Ç–µ—Å—Ç–∏—Ä–æ–≤–∞–Ω–∏—è –≥–µ–Ω–µ—Ä–∞—Ç–æ—Ä–∞"""
    print("ü§ñ –ì–µ–Ω–µ—Ä–∞—Ç–æ—Ä —Å–æ–æ–±—â–µ–Ω–∏–π –≤ –≤–∞—à–µ–º —Å—Ç–∏–ª–µ")
    print("=" * 50)
    print("–í–≤–µ–¥–∏—Ç–µ —Å–æ–æ–±—â–µ–Ω–∏—è —Å–æ–±–µ—Å–µ–¥–Ω–∏–∫–∞ (–∫–∞–∂–¥–æ–µ —Å –Ω–æ–≤–æ–π —Å—Ç—Ä–æ–∫–∏)")
    print("–ö–æ–≥–¥–∞ –∑–∞–∫–æ–Ω—á–∏—Ç–µ, –æ—Å—Ç–∞–≤—å—Ç–µ —Å—Ç—Ä–æ–∫—É –ø—É—Å—Ç–æ–π –∏ –Ω–∞–∂–º–∏—Ç–µ Enter")
    print("–î–ª—è –≤—ã—Ö–æ–¥–∞ –≤–≤–µ–¥–∏—Ç–µ 'exit'")
    print("-" * 50)
    
    try:
        generator = MessageGenerator()
        
        while True:
            print("\nüí¨ –í–≤–µ–¥–∏—Ç–µ –∫–æ–Ω—Ç–µ–∫—Å—Ç:")
            context = []
            
            while True:
                msg = input("–°–æ–æ–±—â–µ–Ω–∏–µ —Å–æ–±–µ—Å–µ–¥–Ω–∏–∫–∞: ").strip()
                if msg.lower() == 'exit':
                    print("üëã –î–æ —Å–≤–∏–¥–∞–Ω–∏—è!")
                    return
                if not msg:
                    break
                context.append(msg)
            
            if not context:
                continue
            
            print(f"\nüìù –ö–æ–Ω—Ç–µ–∫—Å—Ç: {context}")
            
            # –ì–µ–Ω–µ—Ä–∏—Ä—É–µ–º –Ω–µ—Å–∫–æ–ª—å–∫–æ –≤–∞—Ä–∏–∞–Ω—Ç–æ–≤
            print("\nüéØ –ì–µ–Ω–µ—Ä–∏—Ä—É—é –≤–∞—Ä–∏–∞–Ω—Ç—ã –æ—Ç–≤–µ—Ç–æ–≤...")
            responses = generator.generate_multiple_responses(context, num_responses=3)
            
            print("\n‚ú® –í–∞—Ä–∏–∞–Ω—Ç—ã –æ—Ç–≤–µ—Ç–æ–≤:")
            for i, response in enumerate(responses, 1):
                print(f"{i}. {response}")
            
            # –ü–æ–∑–≤–æ–ª—è–µ–º –≤—ã–±—Ä–∞—Ç—å –ª—É—á—à–∏–π
            print("\nüìã –í—ã–±–µ—Ä–∏—Ç–µ –ª—É—á—à–∏–π –æ—Ç–≤–µ—Ç (1-3) –∏–ª–∏ –Ω–∞–∂–º–∏—Ç–µ Enter –¥–ª—è –ø—Ä–æ–ø—É—Å–∫–∞:")
            choice = input("–í–∞—à –≤—ã–±–æ—Ä: ").strip()
            
            if choice in ['1', '2', '3']:
                selected = responses[int(choice) - 1]
                print(f"\n‚úÖ –í—ã–±—Ä–∞–Ω –æ—Ç–≤–µ—Ç: {selected}")
                print("üìã –°–∫–æ–ø–∏—Ä—É–π—Ç–µ –µ–≥–æ –∏ –æ—Ç–ø—Ä–∞–≤—å—Ç–µ –≤ Telegram!")
            else:
                print("‚è≠Ô∏è  –û—Ç–≤–µ—Ç –ø—Ä–æ–ø—É—â–µ–Ω")
    
    except KeyboardInterrupt:
        print("\n\nüëã –î–æ —Å–≤–∏–¥–∞–Ω–∏—è!")
    except Exception as e:
        print(f"\n‚ùå –û—à–∏–±–∫–∞: {e}")
        print("–£–±–µ–¥–∏—Ç–µ—Å—å, —á—Ç–æ –º–æ–¥–µ–ª—å –æ–±—É—á–µ–Ω–∞ –∏ –Ω–∞—Ö–æ–¥–∏—Ç—Å—è –≤ –ø–∞–ø–∫–µ '../dataset/my_dialogpt'")

def batch_mode(context_messages):
    """–ü–∞–∫–µ—Ç–Ω—ã–π —Ä–µ–∂–∏–º –¥–ª—è –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏ –æ–¥–Ω–æ–≥–æ –æ—Ç–≤–µ—Ç–∞"""
    try:
        generator = MessageGenerator()
        response = generator.generate_response(context_messages)
        return response
    except Exception as e:
        logger.error(f"–û—à–∏–±–∫–∞ –≤ –ø–∞–∫–µ—Ç–Ω–æ–º —Ä–µ–∂–∏–º–µ: {e}")
        return f"–û—à–∏–±–∫–∞: {e}"

if __name__ == "__main__":
    print("üöÄ –ó–∞–ø—É—Å–∫ –≥–µ–Ω–µ—Ä–∞—Ç–æ—Ä–∞ —Å–æ–æ–±—â–µ–Ω–∏–π...")
    
    # –ü—Ä–æ–≤–µ—Ä—è–µ–º, –µ—Å—Ç—å –ª–∏ –æ–±—É—á–µ–Ω–Ω–∞—è –º–æ–¥–µ–ª—å
    import os
    if not os.path.exists("../dataset/my_dialogpt"):
        print("‚ùå –û–±—É—á–µ–Ω–Ω–∞—è –º–æ–¥–µ–ª—å –Ω–µ –Ω–∞–π–¥–µ–Ω–∞!")
        print("–°–Ω–∞—á–∞–ª–∞ –æ–±—É—á–∏—Ç–µ –º–æ–¥–µ–ª—å, –∑–∞–ø—É—Å—Ç–∏–≤:")
        print("python train_model_locally.py")
        print("\n–ò–ª–∏ –∑–∞–ø—É—Å—Ç–∏—Ç–µ –≤ –∏–Ω—Ç–µ—Ä–∞–∫—Ç–∏–≤–Ω–æ–º —Ä–µ–∂–∏–º–µ –¥–ª—è —Ç–µ—Å—Ç–∏—Ä–æ–≤–∞–Ω–∏—è")
        choice = input("–ó–∞–ø—É—Å—Ç–∏—Ç—å –∏–Ω—Ç–µ—Ä–∞–∫—Ç–∏–≤–Ω—ã–π —Ä–µ–∂–∏–º? (y/n): ").lower()
        if choice == 'y':
            interactive_mode()
    else:
        interactive_mode() 